# News AI Machine Learning Pipeline Configuration

# Data paths
data:
  raw_path: "../data/raw"
  bronze_path: "../data/bronze"
  silver_path: "../data/silver"
  gold_path: "../data/gold"
  mind_dataset_path: "/Users/sravansridhar/Documents/news_ai/MINDLarge"

# API keys (loaded from environment variables)
api_keys:
  load_from_env: true
  # Keys will be loaded from:
  # - NEWS_API_KEY
  # - OPENAI_API_KEY
  # - HUGGING_FACE_API_KEY

# Preprocessing parameters
preprocessing:
  max_title_length: 64
  max_abstract_length: 512
  min_word_freq: 5
  max_vocab_size: 100000
  use_stemming: true
  use_lemmatization: true
  remove_stopwords: true
  use_spacy: true
  spacy_model: "en_core_web_sm"

# Feature engineering parameters
features:
  text_embeddings:
    model: "sentence-transformers/all-MiniLM-L6-v2"
    batch_size: 256
    max_length: 128
    use_gpu: true
  user_features:
    min_history_length: 5
    temporal_window_days: 7
    recency_weight_decay: 0.85
  content_features:
    use_categories: true
    use_subcategories: true
    use_entities: true
    entity_embedding_dim: 64

# Model parameters
models:
  recommendation:
    model_type: "hybrid"  # Options: content_based, collaborative, hybrid, nrms
    embedding_dim: 128
    hidden_dim: 256
    num_heads: 4
    num_layers: 2
    dropout: 0.1
    learning_rate: 0.001
    batch_size: 256
    epochs: 10
    use_gpu: true
    use_amp: true  # Automatic Mixed Precision

  political_influence:
    base_model: "roberta-base"
    num_classes: 5
    learning_rate: 2e-5
    batch_size: 128
    epochs: 3
    max_length: 256
    use_gpu: true
    use_amp: true

  rhetoric_intensity:
    base_model: "distilbert-base-uncased"
    num_classes: 10
    learning_rate: 3e-5
    batch_size: 128
    epochs: 3
    max_length: 256
    use_gpu: true
    use_amp: true

  information_depth:
    base_model: "bert-base-uncased"
    num_classes: 10
    learning_rate: 2e-5
    batch_size: 128
    epochs: 3
    max_length: 512
    use_gpu: true
    use_amp: true

  sentiment:
    base_model: "distilbert-base-uncased-finetuned-sst-2-english"
    fine_tune: true
    learning_rate: 2e-5
    batch_size: 128
    epochs: 3
    max_length: 256
    use_gpu: true
    use_amp: true

# Evaluation parameters
evaluation:
  recommendation:
    metrics: ["auc", "mrr", "ndcg@5", "ndcg@10", "hit@5", "hit@10"]
    test_size: 0.2
    use_temporal_split: true
  metrics_models:
    metrics: ["accuracy", "precision", "recall", "f1", "auc"]
    cv_folds: 5
    test_size: 0.2

# System resources
resources:
  num_workers: 12  # Number of dataloader workers
  torch_threads: 20
  use_mps: true  # Apple Silicon GPU
  parallel_jobs: 8
  memory_limit_gb: 64
  ray_init: true

# Experiment tracking
experiment_tracking:
  use_mlflow: true
  mlflow_tracking_uri: "./mlruns"
  experiment_name: "news_ai_pipeline"
  log_artifacts: true
  log_models: true
