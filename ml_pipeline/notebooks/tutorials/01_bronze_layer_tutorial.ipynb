{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bronze Layer Processing Tutorial\n",
    "\n",
    "This notebook demonstrates how to process raw MIND dataset files into the Bronze layer of our medallion architecture.\n",
    "\n",
    "## What is the Bronze Layer?\n",
    "\n",
    "The Bronze layer is the first transformation layer in our medallion architecture. It:\n",
    "- Converts raw TSV files to Parquet format\n",
    "- Performs initial data validation and cleansing\n",
    "- Standardizes data schemas\n",
    "- Preserves original data while making it more accessible\n",
    "\n",
    "## Pipeline Steps\n",
    "\n",
    "1. Load the raw TSV files from MINDLarge\n",
    "2. Parse and validate data\n",
    "3. Convert to optimized Parquet format\n",
    "4. Store with metadata\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Set paths\n",
    "BASE_DIR = Path(\"../..\").resolve()\n",
    "MIND_DATASET_PATH = BASE_DIR / \"MINDLarge\"\n",
    "BRONZE_PATH = BASE_DIR / \"ml_pipeline\" / \"data\" / \"bronze\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(BRONZE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Process News Data\n",
    "\n",
    "First, we'll process the news data from TSV to Parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def process_news_data(source_path, output_path, split_name):\n",
    "    \"\"\"Process news data from TSV to Parquet format.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Define the column names for news data\n",
    "    news_columns = [\n",
    "        'news_id', 'category', 'subcategory', 'title', 'abstract', 'url', \n",
    "        'title_entities', 'abstract_entities'\n",
    "    ]\n",
    "    \n",
    "    # Read the TSV file\n",
    "    news_file = source_path / f\"MINDlarge_{split_name}\" / \"news.tsv\"\n",
    "    print(f\"Processing {news_file}...\")\n",
    "    \n",
    "    news_df = pd.read_csv(\n",
    "        news_file, \n",
    "        sep='\\t', \n",
    "        names=news_columns,\n",
    "        quoting=3\n",
    "    )\n",
    "    \n",
    "    # Process entity columns (convert string to JSON)\n",
    "    for col in ['title_entities', 'abstract_entities']:\n",
    "        news_df[col] = news_df[col].apply(\n",
    "            lambda x: json.loads(x) if pd.notna(x) and x.strip() else []\n",
    "        )\n",
    "    \n",
    "    # Create text column by combining title and abstract\n",
    "    news_df['text'] = news_df.apply(\n",
    "        lambda row: f\"{row['title']} {row['abstract']}\" if pd.notna(row['abstract']) else row['title'],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Add metadata columns\n",
    "    news_df['source'] = 'mind'\n",
    "    news_df['split'] = split_name\n",
    "    \n",
    "    # Write to Parquet format\n",
    "    output_file = output_path / f\"news_{split_name}.parquet\"\n",
    "    news_df.to_parquet(output_file, index=False)\n",
    "    \n",
    "    print(f\"Processed {len(news_df)} news articles in {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Output saved to {output_file}\")\n",
    "    \n",
    "    return news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Process Behaviors Data\n",
    "\n",
    "Next, we'll process the user behavior data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def process_behaviors_data(source_path, output_path, split_name):\n",
    "    \"\"\"Process behaviors data from TSV to Parquet format.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Define the column names for behaviors data\n",
    "    behaviors_columns = [\n",
    "        'impression_id', 'user_id', 'time', 'history', 'impressions'\n",
    "    ]\n",
    "    \n",
    "    # Read the TSV file\n",
    "    behaviors_file = source_path / f\"MINDlarge_{split_name}\" / \"behaviors.tsv\"\n",
    "    print(f\"Processing {behaviors_file}...\")\n",
    "    \n",
    "    behaviors_df = pd.read_csv(\n",
    "        behaviors_file, \n",
    "        sep='\\t', \n",
    "        names=behaviors_columns\n",
    "    )\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    behaviors_df['time'] = pd.to_datetime(behaviors_df['time'])\n",
    "    \n",
    "    # Process history column (list of news IDs)\n",
    "    behaviors_df['history'] = behaviors_df['history'].apply(\n",
    "        lambda x: x.split() if pd.notna(x) and x.strip() else []\n",
    "    )\n",
    "    \n",
    "    # Add metadata columns\n",
    "    behaviors_df['split'] = split_name\n",
    "    \n",
    "    # Write to Parquet format\n",
    "    output_file = output_path / f\"behaviors_{split_name}.parquet\"\n",
    "    behaviors_df.to_parquet(output_file, index=False)\n",
    "    \n",
    "    print(f\"Processed {len(behaviors_df)} behavior records in {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Output saved to {output_file}\")\n",
    "    \n",
    "    return behaviors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process Entity Embeddings\n",
    "\n",
    "Finally, we'll convert the entity embeddings to Arrow format for efficient storage and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def process_entity_embeddings(source_path, output_path, split_name):\n",
    "    \"\"\"Process entity embeddings from VEC to Arrow format.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read entity embeddings\n",
    "    entity_file = source_path / f\"MINDlarge_{split_name}\" / \"entity_embedding.vec\"\n",
    "    relation_file = source_path / f\"MINDlarge_{split_name}\" / \"relation_embedding.vec\"\n",
    "    \n",
    "    # Process entity embeddings\n",
    "    if entity_file.exists():\n",
    "        print(f\"Processing {entity_file}...\")\n",
    "        \n",
    "        # Read the first line to get dimensions\n",
    "        with open(entity_file, 'r') as f:\n",
    "            first_line = f.readline().strip().split(' ')\n",
    "            num_entities, embedding_dim = int(first_line[0]), int(first_line[1])\n",
    "        \n",
    "        # Read embeddings\n",
    "        entity_embeddings = {}\n",
    "        with open(entity_file, 'r') as f:\n",
    "            # Skip the first line (header)\n",
    "            next(f)\n",
    "            \n",
    "            # Process each line\n",
    "            for line in f:\n",
    "                tokens = line.strip().split(' ')\n",
    "                entity_id = tokens[0]\n",
    "                embedding = np.array([float(x) for x in tokens[1:]])\n",
    "                entity_embeddings[entity_id] = embedding\n",
    "        \n",
    "        # Convert to Arrow format\n",
    "        entity_ids = list(entity_embeddings.keys())\n",
    "        embeddings = np.stack(list(entity_embeddings.values()))\n",
    "        \n",
    "        # Create Arrow table\n",
    "        table = pa.Table.from_arrays(\n",
    "            [pa.array(entity_ids), pa.array(embeddings.tolist())],\n",
    "            names=['entity_id', 'embedding']\n",
    "        )\n",
    "        \n",
    "        # Write to Arrow file\n",
    "        output_file = output_path / f\"entity_embeddings_{split_name}.arrow\"\n",
    "        with pa.OSFile(str(output_file), 'wb') as sink:\n",
    "            with pa.RecordBatchFileWriter(sink, table.schema) as writer:\n",
    "                writer.write_table(table)\n",
    "        \n",
    "        print(f\"Processed {len(entity_embeddings)} entity embeddings in {time.time() - start_time:.2f} seconds\")\n",
    "        print(f\"Output saved to {output_file}\")\n",
    "    \n",
    "    # Process relation embeddings\n",
    "    if relation_file.exists():\n",
    "        relation_start_time = time.time()\n",
    "        print(f\"Processing {relation_file}...\")\n",
    "        \n",
    "        # Read the first line to get dimensions\n",
    "        with open(relation_file, 'r') as f:\n",
    "            first_line = f.readline().strip().split(' ')\n",
    "            num_relations, embedding_dim = int(first_line[0]), int(first_line[1])\n",
    "        \n",
    "        # Read embeddings\n",
    "        relation_embeddings = {}\n",
    "        with open(relation_file, 'r') as f:\n",
    "            # Skip the first line (header)\n",
    "            next(f)\n",
    "            \n",
    "            # Process each line\n",
    "            for line in f:\n",
    "                tokens = line.strip().split(' ')\n",
    "                relation_id = tokens[0]\n",
    "                embedding = np.array([float(x) for x in tokens[1:]])\n",
    "                relation_embeddings[relation_id] = embedding\n",
    "        \n",
    "        # Convert to Arrow format\n",
    "        relation_ids = list(relation_embeddings.keys())\n",
    "        embeddings = np.stack(list(relation_embeddings.values()))\n",
    "        \n",
    "        # Create Arrow table\n",
    "        table = pa.Table.from_arrays(\n",
    "            [pa.array(relation_ids), pa.array(embeddings.tolist())],\n",
    "            names=['relation_id', 'embedding']\n",
    "        )\n",
    "        \n",
    "        # Write to Arrow file\n",
    "        output_file = output_path / f\"relation_embeddings_{split_name}.arrow\"\n",
    "        with pa.OSFile(str(output_file), 'wb') as sink:\n",
    "            with pa.RecordBatchFileWriter(sink, table.schema) as writer:\n",
    "                writer.write_table(table)\n",
    "        \n",
    "        print(f\"Processed {len(relation_embeddings)} relation embeddings in {time.time() - relation_start_time:.2f} seconds\")\n",
    "        print(f\"Output saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process All Splits\n",
    "\n",
    "Now let's process all the data splits (train, dev, test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Process all splits\n",
    "for split in ['train', 'dev', 'test']:\n",
    "    print(f\"\\nProcessing {split} split\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Process news data\n",
    "    news_df = process_news_data(MIND_DATASET_PATH, BRONZE_PATH, split)\n",
    "    \n",
    "    # Process behaviors data\n",
    "    behaviors_df = process_behaviors_data(MIND_DATASET_PATH, BRONZE_PATH, split)\n",
    "    \n",
    "    # Process entity embeddings\n",
    "    process_entity_embeddings(MIND_DATASET_PATH, BRONZE_PATH, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create User Profiles\n",
    "\n",
    "As a final step, we'll create user profiles from the behaviors data to get a unified view of each user's reading history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_user_profiles():\n",
    "    \"\"\"Create user profiles from behaviors data.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load all behaviors data\n",
    "    behaviors_dfs = []\n",
    "    for split in ['train', 'dev', 'test']:\n",
    "        file_path = BRONZE_PATH / f\"behaviors_{split}.parquet\"\n",
    "        if file_path.exists():\n",
    "            df = pd.read_parquet(file_path)\n",
    "            behaviors_dfs.append(df)\n",
    "    \n",
    "    # Combine behaviors data\n",
    "    if behaviors_dfs:\n",
    "        all_behaviors = pd.concat(behaviors_dfs, ignore_index=True)\n",
    "        \n",
    "        # Group by user_id\n",
    "        user_profiles = []\n",
    "        for user_id, group in all_behaviors.groupby('user_id'):\n",
    "            # Sort by time\n",
    "            group = group.sort_values('time')\n",
    "            \n",
    "            # Combine all history\n",
    "            all_history = []\n",
    "            for history in group['history']:\n",
    "                all_history.extend(history)\n",
    "            \n",
    "            # Remove duplicates while preserving order\n",
    "            seen = set()\n",
    "            unique_history = [x for x in all_history if not (x in seen or seen.add(x))]\n",
    "            \n",
    "            # Create user profile\n",
    "            user_profiles.append({\n",
    "                'user_id': user_id,\n",
    "                'history': unique_history,\n",
    "                'history_length': len(unique_history),\n",
    "                'first_activity': group['time'].min(),\n",
    "                'last_activity': group['time'].max(),\n",
    "                'activity_days': (group['time'].max() - group['time'].min()).days + 1,\n",
    "                'impression_count': len(group)\n",
    "            })\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        user_profiles_df = pd.DataFrame(user_profiles)\n",
    "        \n",
    "        # Write to Parquet format\n",
    "        output_file = BRONZE_PATH / \"user_profiles.parquet\"\n",
    "        user_profiles_df.to_parquet(output_file, index=False)\n",
    "        \n",
    "        print(f\"Created {len(user_profiles_df)} user profiles in {time.time() - start_time:.2f} seconds\")\n",
    "        print(f\"Output saved to {output_file}\")\n",
    "        \n",
    "        return user_profiles_df\n",
    "    else:\n",
    "        print(\"No behaviors data found\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create user profiles\n",
    "user_profiles_df = create_user_profiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Metadata\n",
    "\n",
    "Finally, let's save metadata about the bronze layer processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create metadata\n",
    "metadata = {\n",
    "    \"created_at\": pd.Timestamp.now().isoformat(),\n",
    "    \"mind_dataset_version\": \"MINDLarge\",\n",
    "    \"pipeline_version\": \"1.0\",\n",
    "    \"bronze_files\": {\n",
    "        \"news\": [f\"news_{split}.parquet\" for split in ['train', 'dev', 'test']],\n",
    "        \"behaviors\": [f\"behaviors_{split}.parquet\" for split in ['train', 'dev', 'test']],\n",
    "        \"embeddings\": [\n",
    "            f\"entity_embeddings_{split}.arrow\" for split in ['train', 'dev', 'test']\n",
    "        ] + [f\"relation_embeddings_{split}.arrow\" for split in ['train', 'dev', 'test']],\n",
    "        \"user_profiles\": \"user_profiles.parquet\"\n",
    "    },\n",
    "    \"stats\": {}\n",
    "}\n",
    "\n",
    "# Add statistics for each file\n",
    "for split in ['train', 'dev', 'test']:\n",
    "    news_file = BRONZE_PATH / f\"news_{split}.parquet\"\n",
    "    behaviors_file = BRONZE_PATH / f\"behaviors_{split}.parquet\"\n",
    "    \n",
    "    if news_file.exists():\n",
    "        news_df = pd.read_parquet(news_file)\n",
    "        metadata[\"stats\"][f\"news_{split}\"] = {\n",
    "            \"count\": len(news_df),\n",
    "            \"categories\": news_df['category'].nunique(),\n",
    "            \"subcategories\": news_df['subcategory'].nunique()\n",
    "        }\n",
    "    \n",
    "    if behaviors_file.exists():\n",
    "        behaviors_df = pd.read_parquet(behaviors_file)\n",
    "        metadata[\"stats\"][f\"behaviors_{split}\"] = {\n",
    "            \"count\": len(behaviors_df),\n",
    "            \"users\": behaviors_df['user_id'].nunique()\n",
    "        }\n",
    "\n",
    "# Add user profile stats\n",
    "if user_profiles_df is not None:\n",
    "    metadata[\"stats\"][\"user_profiles\"] = {\n",
    "        \"count\": len(user_profiles_df),\n",
    "        \"avg_history_length\": user_profiles_df['history_length'].mean(),\n",
    "        \"max_history_length\": user_profiles_df['history_length'].max(),\n",
    "        \"avg_activity_days\": user_profiles_df['activity_days'].mean()\n",
    "    }\n",
    "\n",
    "# Save metadata\n",
    "with open(BRONZE_PATH / \"_metadata.json\", 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"Metadata saved to\", BRONZE_PATH / \"_metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've successfully processed the raw MIND dataset into the Bronze layer of our medallion architecture. This layer provides:\n",
    "\n",
    "- Efficient data storage in Parquet and Arrow formats\n",
    "- Standardized schemas across all splits\n",
    "- Initial data cleansing and validation\n",
    "- Aggregated user profiles\n",
    "- Detailed metadata about the processing\n",
    "\n",
    "In the next tutorial, we'll use this Bronze layer data to create the Silver layer with advanced features and embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}